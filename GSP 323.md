## GSP 323 ##

>Task 1 ::=>> Run a simple Dataflow job

1. Crie um bucket no Cloud Storage com nome único para transferir dados para o dataflow job
2. No BigQuery, crie um Dataset com o nome especificado.
3. No terminal do Cloud Shell, execute o comando:

`gsutil cp gs://cloud-training/gsp323/lab.csv .` - Para copiar o arquivo lab.csv

`cat lab.csv .` - Para que o SO leia o arquivo lab.csv

`gsutil cp gs://cloud-training/gsp323/lab.schema .` - Para copiar o arquivo lab.schema

`cat lab.schema .` - Para que o SO leia o arquivo lab.schema

4. Copie a estrutura de colunas da tabela BigQuery Schema para um editor de texto e reserve . (de colchete a colchete)

5. Dentro do Dataset selecione +Create Table, Selecione Table from Google Cloud Storage, format CSV

6. Siga as instruções de preenchimento do laboratório

7. No Schema, selecione Edit as text e cole a estrutura reservada no editor de texto > Create Table

8. Pesquise por Dataflow e selecione Create Job from Template

9. Em dataflow template selecione Text Files on Cloud Storage to BigQuery

10. Siga as instruções de preenchimento do laboratório > Run Job
---------------------------------------------------------------------------------------------------------------------------------------------------------------

>Task 2 :::=> Run a simple Dataproc job

1. Pesquise por Dataproc no GCP > Create CLuster

2. Dentro do Cluster, procure por VM instances e se conecte à instância via SSH

3. Copie o código e cole no terminal da instância para copiar o arquivo data.txt utilizando o comando hdfs para compartilhamento de arquivos entre as VMs do Cluster

4. No Dataproc > Jobs > Submit a Job e siga as instruções do laboratório > Submit

---------------------------------------------------------------------------------------------------------------------------------------------------------------

>Task 3 :::=> Run a simple Dataprep job

1. Abra uma nova guia do Google Cloud Platform e pesquise por DataPrep > faça login na plataforma do Trifacta

2. Clique em import Data e edite o path para o path do arquivo csv com os dados runs.csv > runs -2 > Use in flow

3. Realize as ações solicitadas no laboratório para formatar a tabela > Run

---------------------------------------------------------------------------------------------------------------------------------------------------------------
>Task 4 :::=> AI

1. Utilize o comando abaixo para criar um service account 

`gcloud iam service-accounts create my-natlang-sa \`

`--display-name "my natural language service account"`
  
2. Utilize o comando abaixo para criar o par de chaves de acesso para a service account

`gcloud iam service-accounts keys create ~/key.json \`

`--iam-account my-natlang-sa@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com`
  
3. Utilize o comando abaixo para realizar o download do arquivo speech-request.json

 `wget https://raw.githubusercontent.com/guys-in-the-cloud/cloud-skill-boosts/main/Challenge-labs/Perform%20Foundational%20Data%2C%20ML%2C%20and%20AI%20Tasks%20in%20Google%20Cloud%3A%20Challenge%20Lab/speech-request.json`

4. Utilize o comando abaixo para fazer um request de permissionamento na API relacionada ao speech.json

`curl -s -X POST -H "Content-Type: application/json" --data-binary @speech-request.json \ `
`"https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}" > speech.json`

5. Utilize o comando abaixo para copiar o arquivo speech.json para o local especificado no laboratório

`gsutil cp speech.json gs://$DEVSHELL_PROJECT_ID-marking/<changefilename>`

6. Utilize o comando abaixo para solicitar o Natural Language que analise a sentença mencionada:

`gcloud ml language analyze-entities --content="Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat." > language.json`

7. Utilize o comando abaixo para copiar o arquivo language.json para o local especificado no laboratório

`gsutil cp language.json gs://$DEVSHELL_PROJECT_ID-marking/<changefilename>`

8. Utilize o comando abaixo para realizar o download do arquivo video-intelligence-request.hson

`wget https://github.com/guys-in-the-cloud/cloud-skill-boosts/blob/main/Challenge-labs/Perform%20Foundational%20Data%2C%20ML%2C%20and%20AI%20Tasks%20in%20Google%20Cloud:%20Challenge%20Lab/video-intelligence-request.json`

9. Utilize o comando abaixo para fazer um request de permissionamento na API relacionada ao video.json

`curl -s -H 'Content-Type: application/json' \`

`-H 'Authorization: Bearer '$(gcloud auth print-access-token)'' \`

`'https://videointelligence.googleapis.com/v1/videos:annotate' \`

 `-d @video-intelligence-request.json  > video.json`
    
10. Utilize o comando abaixo para copiar o arquivo video.json para o local especificado no laboratório
    
 `gsutil cp video.json gs://$DEVSHELL_PROJECT_ID-marking/<changefilename>`
